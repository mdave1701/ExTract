{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization Using Bag of Words and TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read GloVe Embeddings For TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reads the GloVe embeddings that will be used for the TextRank model later\n",
    "def read_GloVe_embeddings(path='glove.6B.100d.txt'):\n",
    "    embeddings = {}\n",
    "    file = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings[word] = coefs\n",
    "    file.close()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "embeddings = read_GloVe_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Input and Parse the Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets the article links and number of sentences to include in the summary from the user\n",
    "def get_input():\n",
    "    article_links = []\n",
    "    print(\"Please enter the article links you wish to summarize:\")\n",
    "    print(\"Enter -1, if you are done entering links.\")\n",
    "    while True:\n",
    "        link = input()\n",
    "        if link == \"-1\":\n",
    "            break\n",
    "        article_links.append(link)\n",
    "            \n",
    "    print(\"\\nPlease enter the number of sentences you want in the summary:\")\n",
    "    num_sents = int(input())\n",
    "    return article_links, num_sents\n",
    "\n",
    "# This function parses the raw xml text into string format\n",
    "def parse_articles(articles):\n",
    "    article_text_lst = []\n",
    "    for article in articles:\n",
    "        raw_article = urllib.request.urlopen(article).read()\n",
    "        parsed_article = bs.BeautifulSoup(raw_article, 'lxml')\n",
    "        paragraphs = parsed_article.find_all('p')\n",
    "        article_text = \"\"\n",
    "        for p in paragraphs:\n",
    "            article_text += p.text\n",
    "        article_text_lst.append(article_text)\n",
    "    \n",
    "    return ' '.join(article_text_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function preproccesses the text for the bag of words model\n",
    "def preprocess_text_wf(corpus):\n",
    "    corpus = re.sub(r'\\[[0-9]*\\]', ' ', corpus)     # removes any brackets with numbers in them (for citations)\n",
    "    corpus = re.sub(r'\\s+', ' ', corpus) # removes any extra spaces\n",
    "    formatted_corpus = re.sub(r'[^a-zA-Z]', ' ', corpus)   # removes any characters that are non-alphabetic\n",
    "    formatted_corpus = re.sub(r'\\s+', ' ', formatted_corpus) # removes any extra spaces\n",
    "    \n",
    "    # tokenize the original corpus which will be used to generate the summary\n",
    "    sentences_wf = nltk.sent_tokenize(corpus)        \n",
    "    return formatted_corpus, sentences_wf\n",
    "\n",
    "# This function preproccesses the text for the TextRank model\n",
    "def preprocess_text_textrank(corpus):\n",
    "    # get rid of citation brackets and extra spaces from the corpus\n",
    "    corpus = re.sub(r'\\[[0-9]*\\]', ' ', corpus)\n",
    "    corpus = re.sub(r'\\s+', ' ', corpus)\n",
    "    \n",
    "    # tokenize the original corpus which will be used to generate the summary\n",
    "    sentences_glove = nltk.sent_tokenize(corpus)\n",
    "    formatted_sents = []\n",
    "    \n",
    "    # remove non-alphabetic characters for computation of similarity matrix\n",
    "    for s in sentences_glove:\n",
    "        formatted_sent = re.sub(r'[^a-zA-Z]', ' ', s)\n",
    "        formatted_sent = re.sub(r'\\s+', ' ', formatted_sent)\n",
    "        formatted_sents.append(formatted_sent.lower())\n",
    "        \n",
    "    return sentences_glove, formatted_sents\n",
    "    \n",
    "# This function removes stopwords from a give sentence\n",
    "def remove_stopwords(sent):\n",
    "    stopWords = nltk.corpus.stopwords.words('english')\n",
    "    new_sent = \" \".join([word for word in sent if word not in stopWords])\n",
    "    return new_sent    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function build a normalized frequency distribution of words in the corpus\n",
    "def build_freqDist(corpus):\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "    word_freqs = nltk.FreqDist(words)\n",
    "    max_freq = max(word_freqs.values())\n",
    "    word_freqs_normalized = {k:v/max_freq for k,v in word_freqs.items()}\n",
    "      \n",
    "    return word_freqs_normalized\n",
    "\n",
    "# This function builds a dictionary of sentence scores\n",
    "def calculate_sent_scores(wordFreqs, sentence_list):\n",
    "    sent_scores = {}\n",
    "    \n",
    "    # looping through the original sentence list to get original sentences in the summary\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in wordFreqs.keys():\n",
    "                if len(sent.split()) < 35:   # only grab sentences with less than 35 words in them\n",
    "                    if sent not in sent_scores.keys():\n",
    "                        sent_scores[sent] = wordFreqs[word]\n",
    "                    else:\n",
    "                        sent_scores[sent] += wordFreqs[word]\n",
    "    \n",
    "    return sent_scores\n",
    "\n",
    "# This function ranks the sentences in descending order of sentence score and \n",
    "# returns a summary with desired number of sentences\n",
    "def generate_summary_wordFreq(sentScores, numSents):\n",
    "    sorted_scores = sorted(sentScores.items(), key=lambda x: x[1], reverse=True)\n",
    "    sents_scored = [k for k,v in sorted_scores]\n",
    "    summary = ' '.join(sents_scored[:numSents])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function vectorizes the sentences\n",
    "def vectorize_sentences(clean_sents, embeddings):\n",
    "    sent_vecs = []\n",
    "    \n",
    "    for sent in clean_sents:\n",
    "        if len(sent) > 0:\n",
    "            # gets the embedding values for the words in the sentence and add them\n",
    "            # then normalize it by the len of the sent\n",
    "            vals = sum([embeddings.get(word, np.zeros((100,)))        \n",
    "                    for word in sent.split()])/(len(sent.split()))\n",
    "        else:\n",
    "            vals = np.zeros((100,))\n",
    "        \n",
    "        sent_vecs.append(vals)\n",
    "        \n",
    "    return sent_vecs\n",
    "\n",
    "# This function computes the similarity matrix for the PageRank model\n",
    "def compute_similarity_matrix(sent_vecs, n):\n",
    "    sim_mat = np.zeros([n, n])\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sent_vecs[i].reshape(1,100), sent_vecs[j].reshape(1,100))[0,0]\n",
    "    \n",
    "    return sim_mat\n",
    "\n",
    "# This function calculates the PageRank scores for the sentences\n",
    "def get_Pagerank_scores(sim_mat):\n",
    "    graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(graph)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# This function ranks the sentences and generates the summary\n",
    "# of the desired number of sentences\n",
    "def generate_summary_GloVe(scores, sents, numSents):\n",
    "    ranked_sents_scores = sorted(((scores[i], s) for i, s in enumerate(sents)), reverse=True)\n",
    "    ranked_sents = [s for i, s in ranked_sents_scores if len(s.split()) < 35]   # only select sentences with <35 words\n",
    "    summary = ' '.join(ranked_sents[:numSents])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the article links you wish to summarize:\n",
      "Enter -1, if you are done entering links.\n",
      "https://en.wikipedia.org/wiki/Virtual_reality#Etymology\n",
      "-1\n",
      "\n",
      "Please enter the number of sentences you want in the summary:\n",
      "10\n",
      "\n",
      "Bag of Words Model:\n",
      "\n",
      "Currently standard virtual reality systems use either virtual reality headsets or multi-projected environments to generate realistic images, sounds and other sensations that simulate a user's physical presence in a virtual environment. Simulated reality is a hypothetical virtual reality as truly immersive as the actual reality, enabling an advanced lifelike experience or even virtual eternity. In projector-based virtual reality, modeling of the real environment plays a vital role in various virtual reality applications, such as robot navigation, construction modeling, and airplane simulation. The system enabled the overlay of physically real 3D virtual objects registered with a user's direct view of the real world, producing the first true augmented reality experience enabling sight, sound, and touch. One method by which virtual reality can be realized is simulation-based virtual reality. Virtual reality sickness (also known as cybersickness) occurs when a person's exposure to a virtual environment causes symptoms that are similar to motion sickness symptoms. Augmented reality (AR) is a type of virtual reality technology that blends what the user sees in their real surroundings with digital content generated by computer software. With avatar image-based virtual reality, people can join the virtual environment in the form of real video as well as an avatar. The original LEEP system was redesigned for NASA's Ames Research Center in 1985 for their first virtual reality installation, the VIEW (Virtual Interactive Environment Workstation) by Scott Fisher. The Virtual Reality Modelling Language (VRML), first introduced in 1994, was intended for the development of \"virtual worlds\" without dependency on headsets.\n",
      "\n",
      "\n",
      "TextRank Model:\n",
      "\n",
      "Many modern first-person video games can be used as an example, using various triggers, responsive characters, and other such interactive devices to make the user feel as though they are in a virtual world. The system enabled the overlay of physically real 3D virtual objects registered with a user's direct view of the real world, producing the first true augmented reality experience enabling sight, sound, and touch. The term \"virtual\" has been used in the computer sense of \"not physically existing but made to appear by software\" since 1959. That same year, Louis Rosenberg created the virtual fixtures system at the U.S. Air Force's Armstrong Labs using a full upper-body exoskeleton, enabling a physically realistic mixed reality in 3D. One can participate in the 3D distributed virtual environment as form of either a conventional avatar or a real video. A person using virtual reality equipment is able to look around the artificial world, move around in it, and interact with virtual features or items. The additional software-generated images with the virtual scene typically enhance how the real surroundings look in some way. The concept was later adapted into the personal computer-based, 3D virtual world program Second Life. In 1968, Ivan Sutherland, with the help of his students including Bob Sproull, created what was widely considered to be the first head-mounted display system for use in immersive simulation applications. This marked the first major commercial release of sensor-based tracking, allowing for free movement of users within a defined space.\n"
     ]
    }
   ],
   "source": [
    "# get the links and parse the articles\n",
    "links, num_sents = get_input()\n",
    "corpus = parse_articles(links)\n",
    "\n",
    "# Bag of words model\n",
    "formatted_corpus_wf, sentences_wf = preprocess_text_wf(corpus)                      # preprocess the text\n",
    "sentences_wf_no_sw = remove_stopwords(nltk.word_tokenize(formatted_corpus_wf))      # remove stropwords\n",
    "weighted_wordFreqs = build_freqDist(sentences_wf_no_sw)                             # build frequency distribution\n",
    "sent_scores = calculate_sent_scores(weighted_wordFreqs, sentences_wf)               # calculate sentence scores\n",
    "summary_word_freq = generate_summary_wordFreq(sent_scores, num_sents)               # generate the summary\n",
    "print(\"\\nBag of Words Model:\\n\")\n",
    "print(summary_word_freq)\n",
    "\n",
    "# TextRank model\n",
    "orig_sents, clean_sents = preprocess_text_textrank(corpus)                          # preprocess the text \n",
    "clean_sents_no_sw = [remove_stopwords(s.split()) for s in clean_sents]              # remove stropwords\n",
    "sentence_vecs = vectorize_sentences(clean_sents_no_sw, embeddings)                  # vectorize the sentencs\n",
    "similarity_mat = compute_similarity_matrix(sentence_vecs, len(orig_sents))          # compute the similarity matrix\n",
    "pageRank_scores = get_Pagerank_scores(similarity_mat)                               # compute the pagerank scores\n",
    "summary_glove = generate_summary_GloVe(pageRank_scores, orig_sents, num_sents)      # generate the summary\n",
    "print(\"\\n\\nTextRank Model:\\n\")\n",
    "print(summary_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Due to the increase in data and just the sheer amount of information one needs to keep up with to stay updated with the world, it is essential to summarize news articles or any long texts to save time and understand the essential aspects of the text quickly. The purpose of this project is to do precisely this. This text summarizer takes a corpus of text(s) and gives the user a concise summary in the number of sentences the user specifies.\n",
    "\n",
    "### Functionality\n",
    "\n",
    "This text summarizer allows the user to enter either one or more article links and produces a combined summary of the texts for the user to read. This can be used for any text that the user enters, but it works best if the text entered is short to medium in length. If it is too long then the summary may not be reflective of the main ideas of the text. The application also gives the user two summaries generated using two different models, namely, Bag of Words and TextRank. \n",
    "\n",
    "The Bag of Words model generates a very consise summary, but if in the odd case that it does not generate a relevant summary, then the user has the option of looking at the summary generated using the TextRank model. Alternatively, the user can read both summaries to guage an enhanced sense of the texts.\n",
    "\n",
    "### Challenges Faced\n",
    "\n",
    "I did not face many challenges, but one of the challenges I faced was that the summaries were appearing with the preprocessed sentences, i.e. without stopwords and non-alphabetic characters. It took me some time to debug and see that the corpus that I was tokenizing was the preprocessed corpus and not the corpus with the original text.\n",
    "\n",
    "Another issue that I was facing was associating the sentences with their respective PageRank scores since the scores were stored in a dictionary with integer-based indices, but then I noticed that they correspond to the original sentences list, so I just zipped them together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
